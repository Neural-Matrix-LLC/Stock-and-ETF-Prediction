{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c0e038-57f5-4962-bc98-a3458f5495cc",
   "metadata": {},
   "source": [
    "# Prediction of Volatiltiy with Deep Learning and GARCH model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf015b-fe14-47ec-9c2f-ff95d10ca5bd",
   "metadata": {},
   "source": [
    "This notebook aims to predict market volatility using machine learning integrated with an econometrics model.\n",
    "\n",
    "Data from ```'HKEX', 'NYSE', 'NASDAQ', 'AMEX'``` during ```1998-01-01 to 2021-09-07``` was employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957a2c3-acd1-43e7-8718-0b1ed54cbc8e",
   "metadata": {},
   "source": [
    "## Volatiltiy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d32016-d769-43b5-8522-59babff297be",
   "metadata": {},
   "source": [
    "The volatility $\\sigma$, of a stock is a measure of our uncertainty about the returns provided by the stock. It can be defined as the standard deviation of the return provided by the stock. Volatility can be measured in several ways:\n",
    "1. Historical volatility is calculated from historical data of a stock price.\n",
    "2. Implied volatility is calculated from prices observed in the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21b9b2-232e-46f2-bbe1-c9b1153089b3",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573771a-28d7-4d92-aa42-88a734e9780f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca704a3-2c0d-4c89-b442-7b4ff13b91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: Darwin x86_64\n",
      "\n",
      "3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:59:00) \n",
      "[Clang 11.1.0 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import mysql.connector\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"mysql.env\")\n",
    "\n",
    "# Visualization + diagnositic\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "from scipy.stats import probplot, shapiro\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "# Model\n",
    "from arch import arch_model\n",
    "from arch.__future__ import reindexing\n",
    "\n",
    "\n",
    "# Performance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Machine: {} {}\\n'.format(os.uname().sysname,os.uname().machine))\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903850e-016b-458b-a76e-20e597cd3337",
   "metadata": {},
   "source": [
    "# List of Stocks and ETFs\n",
    "Provided by Thomas Choi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a8a8034-3331-4de3-a853-64746e2258b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list = pd.read_csv(\"stocks_and_etfs/stock_list.csv\")\n",
    "etf_list = pd.read_csv(\"stocks_and_etfs/etf_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b34b8527-f710-4750-85c3-6f1f9dfd3e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DKNG'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_symbol = stock_list.iloc[8,0]\n",
    "stock_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6b114-3d8f-4bd7-b43c-a39ad72575d4",
   "metadata": {},
   "source": [
    "## MySQL connection\n",
    "Choosing one stock from SQL query to reduce query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535f4df-1189-4de1-88ea-f86ef785c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST=os.environ.get(\"HOST\")\n",
    "PORT=os.environ.get(\"PORT\")\n",
    "USER=os.environ.get(\"USER\")\n",
    "PASSWORD=os.environ.get(\"PASSWORD\")\n",
    "\n",
    "try: \n",
    "    conn = mysql.connector.connect(\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        database=\"GlobalMarketData\"\n",
    "    )\n",
    "    query = f\"SELECT Date, Close, Open, High, Low, Volume from histdailyprice3 WHERE Symbol='{stock_symbol}';\"\n",
    "    histdailyprice3 = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    conn.close()\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a46a4e-bef7-449b-9a8f-db6507550cbb",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f646887-1663-4c30-9618-7003b82b711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = histdailyprice3.copy()\n",
    "df.set_index(\"Date\", drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f42c7-b59c-4138-bd66-de87d83811c5",
   "metadata": {},
   "source": [
    "## Closing Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0591ef1-e55b-4b49-8b2d-b70d30b5415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot(title=f'Closing Price from 1998-01-01 to 2021-09-07')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c6de8-860c-483d-a40a-c520dd478f5c",
   "metadata": {},
   "source": [
    "## Calculate Daily Returns\n",
    "The formula for return is the following, where $u$ denotes return and $S$ denotes the price of an underlying asset:\n",
    "$$ u_n = \\frac{S_n - S_{n-1}}{S_{n-1}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc067da-04a4-49ae-b66d-5853ce0e439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pct_change'] = 100 * df['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f0c2f-cb48-4364-bd85-fe1dd95552f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(12)\n",
    "plt.plot(abs(df['pct_change']), label = 'Percent change')\n",
    "plt.axhline(y=threshold, color='g', linestyle='-')\n",
    "plt.title('Percent change in Closing price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606a0e8-0e15-48d1-a61f-31410711d4dc",
   "metadata": {},
   "source": [
    "## Calculate daily, monthly, and annual volatitily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857b6dc-dbef-4548-a87e-8b19e13c4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_volatility = df['pct_change'].std()\n",
    "print('Daily volatility: ', '{:.2f}%'.format(daily_volatility))\n",
    "\n",
    "monthly_volatility = math.sqrt(21) * daily_volatility\n",
    "print ('Monthly volatility: ', '{:.2f}%'.format(monthly_volatility))\n",
    "\n",
    "annual_volatility = math.sqrt(252) * daily_volatility\n",
    "print ('Annual volatility: ', '{:.2f}%'.format(annual_volatility ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4e28a-0838-4049-b95d-179555d691ba",
   "metadata": {},
   "source": [
    "## ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f081bec-fdb5-4277-9877-91fdae439d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create acf plot\n",
    "plot_acf(df[\"pct_change\"], lags=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67abd3d-79f5-4f42-b93d-b220d30f3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pacf plot\n",
    "plot_pacf(df[\"pct_change\"], lags=10, method='ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad37b9-58a5-4286-8c3d-f11222951398",
   "metadata": {},
   "source": [
    "## Ljung-Box Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90868017-d899-4aee-91d1-cf98efbbc8be",
   "metadata": {},
   "source": [
    "Ljung-Box is a test for autocorrelation that we can use in tandem with our ACF and PACF plots. The Ljung-Box test takes our data, optionally either lag values to test, or the largest lag value to consider, and whether to compute the Box-Pierce statistic. Ljung-Box and Box-Pierce are two similar test statisitcs, $Q$ , that are compared against a chi-squared distribution to determine if the series is white noise. We might use the Ljung-Box test on the residuals of our model to look for autocorrelation, ideally our residuals would be white noise.\n",
    "\n",
    "- $H_o$ : The data are independently distributed, no autocorrelation.\n",
    "- $H_a$ : The data are not independently distributed; they exhibit serial correlation.\n",
    "The Ljung-Box with the Box-Pierce option will return, for each lag, the Ljung-Box test statistic, Ljung-Box p-values, Box-Pierce test statistic, and Box-Pierce p-values.\n",
    "\n",
    "If  p<Î±  (0.05) we reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5cc06-f757-437d-b66b-67018f270b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljung_res = acorr_ljungbox(df['pct_change'], lags= 40, boxpierce=True)\n",
    "ljung_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac14d8e-3889-4e02-a46a-26ee606a47eb",
   "metadata": {},
   "source": [
    "## Time Series Plot Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52308b2-a3a1-4049-852e-49dad76ee174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_plot(residuals, stan_residuals, lags=50):\n",
    "    residuals.plot(title='GARCH Residuals', figsize=(15, 10))\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "    ax[0].set_title('GARCH Standardized Residuals KDE')\n",
    "    ax[1].set_title('GARCH Standardized Resduals Probability Plot')    \n",
    "    residuals.plot(kind='kde', ax=ax[0])\n",
    "    probplot(stan_residuals, dist='norm', plot=ax[1])\n",
    "    plt.show()\n",
    "    acf = plot_acf(stan_residuals, lags=lags)\n",
    "    pacf = plot_pacf(stan_residuals, lags=lags, method=\"ywm\")\n",
    "    acf.suptitle('GARCH Model Standardized Residual Autocorrelation', fontsize=20)\n",
    "    acf.set_figheight(5)\n",
    "    acf.set_figwidth(15)\n",
    "    pacf.set_figheight(5)\n",
    "    pacf.set_figwidth(15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52361afd-6166-4789-a48c-959b195f8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch = arch_model(df['pct_change'], vol='GARCH', p=1, q=1, dist='normal')\n",
    "fgarch = garch.fit(disp='off') \n",
    "resid = fgarch.resid\n",
    "st_resid = np.divide(resid, fgarch.conditional_volatility)\n",
    "ts_plot(resid, st_resid)\n",
    "fgarch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c7fb4-6a6b-4f46-89f1-87ae9eee70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_test = het_arch(resid, nlags=50)\n",
    "shapiro_test = shapiro(st_resid)\n",
    "\n",
    "print(f'Lagrange mulitplier p-value: {arch_test[1]}')\n",
    "print(f'F test p-value: {arch_test[3]}')\n",
    "print(f'Shapiro-Wilks p-value: {shapiro_test[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa56672-d8ca-40c3-b9f2-0deeb258dab3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GARCH model\n",
    "Generalized Auto Regressive Conditional Heteroskedasticity <br>\n",
    "GARCH(1,1) is a widely used econmetric model to estimate historical volatility.\n",
    "\n",
    "GARCH estimates the historical volatility in Day $n$ with:\n",
    "- **Rate of return** in Day n-1, denoted by $u$\n",
    "- **Volatiltiy** in Day n-1, denoted by $\\sigma$ ($\\sigma^2$ is variance)\n",
    "\n",
    "The three coefficients in the model:\n",
    "- **alpha**, coefficient of $u^2$\n",
    "- **beta**, coefficient of $\\sigma^2$\n",
    "- **omega**, constant <br>\n",
    "\n",
    "The formula for estiamting historical volatility is:\n",
    "$$\\sigma^2_n = \\omega + \\alpha u^2_{n-1} + \\beta \\sigma^2_{n-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353c061-8a5a-47b2-975d-d3d2f7ca0eb6",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d2b56-aef5-4b60-8890-73d409d5106a",
   "metadata": {},
   "source": [
    "To measure how well our ARCH/GARCH model fit our data:\n",
    "1. Autocorrelation in the standardized residuals using Ljung-Box test.\n",
    "2. ARCH effects (conditional heteroskedasticity) in the residuals using Engle's ARCH test on the residuals.\n",
    "3. Normal distribution in the standardized residuals, we can use the Shapiro-Wilk test, Q-Q plot, and if larger n- the Jarque-Bera test to see if our data approaches normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a13a3-88c7-4abb-8142-08d933b1a93f",
   "metadata": {},
   "source": [
    "### Engle's ARCH test\n",
    "Deterin whether our ARCH model has captured the conditional heteroskedasticity of our time series.\n",
    "- $H_o$ : The squared residuals are a sequence of white noise- the residuals are homoscedastic.\n",
    "- $H_a$ : The squared residuals could not be fitted with a linear regression model and exhibit heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6be485-530b-4c40-b784-1f995eefc0f8",
   "metadata": {},
   "source": [
    "### Shapiro-Wilks test\n",
    "Evaluates a data sample and quantifies how likely the data was drawn from a Gaussian distribution.\n",
    "- $H_o$ : The data is normally distributed\n",
    "- $H_a$: The data is not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3fcd9-7e6d-475f-99d2-2aeaa8413517",
   "metadata": {},
   "source": [
    "### Jarque-Bera Test\n",
    "A type of lagrange multiplier test for normality. Usually used for large data sets because other normality tests are not reliable when n is large, Shapiro-Wilk is not reliable with n more than 2,000. Jarque-Bera specifically matches skewness and kurtosis to a normal distribution.\n",
    "- $H_o$ : The data is normally distributed\n",
    "- $H_a$: The data is not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4223a66-a2a5-4bbd-a5da-5d1457b4c430",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62538b43-964b-4171-843f-d76e81b8cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(data, p_rng, q_rng):\n",
    "    top_score, top_results = float('inf'), None\n",
    "    top_models = []\n",
    "    for p in p_rng:\n",
    "        for q in q_rng:\n",
    "            try:\n",
    "                model = arch_model(data, vol='GARCH', p=p, q=q, dist='normal')\n",
    "                model_fit = model.fit(disp='off')\n",
    "                resid = model_fit.resid\n",
    "                st_resid = np.divide(resid, model_fit.conditional_volatility)\n",
    "                results = evaluate_model(resid, st_resid)\n",
    "                results['AIC'] = model_fit.aic\n",
    "                results['params']['p'] = p\n",
    "                results['params']['q'] = q\n",
    "                if results['AIC'] < top_score: \n",
    "                    top_score = results['AIC']\n",
    "                    top_results = results\n",
    "                elif results['LM_pvalue'][1] is False:\n",
    "                    top_models.append(results)\n",
    "            except:\n",
    "                continue\n",
    "    top_models.append(top_results)\n",
    "    return top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3dc504-934b-47a7-90f6-f14b3c355c3a",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873942de-8846-4b28-ac64-2bcbe53e7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(residuals, st_residuals, lags=50):\n",
    "    results = {\n",
    "        'LM_pvalue': None,\n",
    "        'F_pvalue': None,\n",
    "        'SW_pvalue': None,\n",
    "        'AIC': None,\n",
    "        'params': {'p': None, 'q': None}\n",
    "    }\n",
    "    arch_test = het_arch(residuals, nlags=lags)\n",
    "    shap_test = shapiro(st_residuals)\n",
    "    # We want falsey values for each of these hypothesis tests\n",
    "    results['LM_pvalue'] = [arch_test[1], arch_test[1] < .05]\n",
    "    results['F_pvalue'] = [arch_test[3], arch_test[3] < .05]\n",
    "    results['SW_pvalue'] = [shap_test[1], shap_test[1] < .05]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e257085-2a4d-420e-afd8-bf2051b996c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rng = range(0,30)\n",
    "q_rng = range(0,40)\n",
    "df['dif_pct_change'] = df['pct_change']#.diff()\n",
    "\n",
    "# Time the grid search\n",
    "start = timeit.default_timer()\n",
    "\n",
    "top_models = gridsearch(df['dif_pct_change'], p_rng, q_rng)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "print(top_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ab28a-f77f-4637-b9a3-a81243db505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = top_models[0]['params']['p']\n",
    "q = top_models[0]['params']['q']\n",
    "garch = arch_model(df['pct_change'], vol='GARCH', p=p, q=q, dist='normal')\n",
    "fgarch = garch.fit(disp='off') \n",
    "resid = fgarch.resid\n",
    "st_resid = np.divide(resid, fgarch.conditional_volatility)\n",
    "ts_plot(resid, st_resid)\n",
    "arch_test = het_arch(resid, nlags=50)\n",
    "shapiro_test = shapiro(st_resid)\n",
    "print(f'Lagrange mulitplier p-value: {arch_test[1]}')\n",
    "print(f'F test p-value: {arch_test[3]}')\n",
    "print(f'Shapiro-Wilks p-value: {shapiro_test[1]}')\n",
    "fgarch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b329ffa-8918-49ae-b3be-e5eef812859d",
   "metadata": {},
   "source": [
    "# Predict Stock Volatilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a4d93-9111-4bc5-98d9-6b473a658517",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model = arch_model(df['pct_change'], p = p, q = q,\n",
    "                      mean = 'constant', vol = 'GARCH', dist = 'normal')\n",
    "\n",
    "gm_result = garch_model.fit(disp='off')\n",
    "print(gm_result.params)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "gm_forecast = gm_result.forecast(horizon = 5)\n",
    "print(gm_forecast.variance[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecc473-059e-4e53-9801-5d7ec4a22cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_predictions = []\n",
    "test_size = 365\n",
    "for i in range(test_size):\n",
    "    train = df['pct_change'][:-(test_size-i)]\n",
    "    model = arch_model(train, p=p, q=q)\n",
    "    model_fit = model.fit(disp='off')\n",
    "    pred = model_fit.forecast(horizon=1)\n",
    "    rolling_predictions.append(np.sqrt(pred.variance.values[-1,:][0]))\n",
    "    \n",
    "rolling_predictions = pd.Series(rolling_predictions, index=df['pct_change'].index[-365:])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(rolling_predictions)\n",
    "plt.title('Rolling Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889fe9a-2dfb-4ac4-85d6-8b68204c1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(abs(df['pct_change'][-365:]))\n",
    "plt.plot(rolling_predictions)\n",
    "plt.axhline(y=threshold, color='g', linestyle='-')\n",
    "plt.title('Volatility Prediction - Rolling Forecast')\n",
    "plt.legend(['True Voltatilty', 'Predicted Volatility'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b3c9e-a17f-41ee-816c-578ec2cce843",
   "metadata": {},
   "source": [
    "# INTEGRATE With DEEP LEARNING\n",
    "AFTER UP or DOWN model\n",
    "Model Stacking <br>\n",
    "Ensemble stacking\n",
    "\n",
    "TODO: Running on the 200-300 stocks <br>\n",
    "\n",
    "Relationship between volatiltiy between up and down <br>\n",
    "Up = low voltatilty, down = high volatility <br>\n",
    "\n",
    "Consider performance on different stocks, may have to choose which stock this model is good at.\n",
    "\n",
    "TODO: LSTM /BILSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57d7bc-249e-4a5c-b738-7c0195ebfb86",
   "metadata": {},
   "source": [
    "# Performance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f6422-1e7c-4636-9f82-94b35d01e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(rolling_predictions >= 2)\n",
    "y_true = np.array(abs(df['pct_change'][-365:]) >= 2)\n",
    "\n",
    "# Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(\"tn:\", tn, \"fp:\", fp, \"fn:\", fn, \"tp:\", tp)\n",
    "\n",
    "# Precision score\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "print(\"Macro precision score:\", precision_macro)\n",
    "print(\"Micro precision score:\", precision_micro)\n",
    "\n",
    "# f1 score\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "print(\"Macro f1 score:\", f1_macro)\n",
    "print(\"Micro f1 score:\", f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf0437-d53d-4bd7-9e82-5fe2914601b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Performance on multiple stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d968b-23c5-44d1-ad0c-5d2230313e78",
   "metadata": {},
   "source": [
    "## TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fb724-9db0-48fd-85cf-4c15533d2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'reports/GARCH performance on stocks and etfs'\n",
    "with open(filename, 'a') as f:\n",
    "    f.write(f\"\\n{stock_symbol}:\\nMacro precision score: {precision_macro}\\nMicro precision score: {precision_micro}\\nMacro f1 score: {f1_macro}\\nMicro f1 score: {f1_micro}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ae8a2-4a4b-4e95-ae9d-c55497d0fb4d",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7b0bd-a669-4eec-a80c-df8d8f3d62ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_performance = pd.read_csv(\"reports/GARCH_performance.csv\")\n",
    "garch_performance = garch_performance.drop([\"Unnamed: 0\"], axis=1)\n",
    "garch_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b2061-ac3e-4457-b433-b02f1b97177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_performance.loc[len(garch_performance.index)] = [stock_symbol, precision_macro, precision_micro, f1_macro, f1_micro]\n",
    "garch_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed826530-b980-47b7-8332-c3817f0c9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_performance.to_csv(\"reports/GARCH_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3074389-af6c-45a0-aa06-ab6f34988922",
   "metadata": {},
   "source": [
    "# Work Cited\n",
    "Liu, Wing Ki, and Mike K. P. So. \"A GARCH Model with Artifical Neural Networks.\" 20 Oct 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6372a4-e6ff-4647-b413-9f67093bb467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
